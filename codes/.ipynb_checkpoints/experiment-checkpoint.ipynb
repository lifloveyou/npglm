{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import threading\n",
    "import numpy as np\n",
    "from codes.models import Model\n",
    "from codes.models.ExpGlm import ExpGlm\n",
    "from codes.models.WblGlm import WblGlm\n",
    "from codes.models.NpGlm import NpGlm\n",
    "from codes.models.RayGlm import RayGlm\n",
    "from codes.features.delicious.extraction import run as delicious_run\n",
    "from codes.features.utils import timestamp_delta_generator\n",
    "from codes.features.autoencoder import encode\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, mean_squared_log_error, median_absolute_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(dist):\n",
    "    return {\n",
    "        'np': NpGlm(),\n",
    "        'wbl': WblGlm(),\n",
    "        'exp': ExpGlm(),\n",
    "        'ray': RayGlm(),\n",
    "        'pow': PowGlm(),\n",
    "        'gom': GomGlm()\n",
    "    }[dist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_c_index(T_true, T_pred, Y):\n",
    "    total_number_of_pairs = 0\n",
    "    number_of_correct_predictions = 0\n",
    "\n",
    "    for i in range(len(T_true)):\n",
    "        for j in range(len(T_true) - 1, i, -1):\n",
    "            if Y[i] != 0 or Y[j] != 0:  # if one or both of the samples are in observation window\n",
    "                total_number_of_pairs += 1\n",
    "                if T_true[i] > T_true[j] and T_pred[i] > T_pred[j]:\n",
    "                    number_of_correct_predictions += 1\n",
    "                if T_true[i] < T_true[j] and T_pred[i] < T_pred[j]:\n",
    "                    number_of_correct_predictions += 1\n",
    "                if T_true[i] == T_true[j] and T_pred[i] == T_pred[j]:\n",
    "                    number_of_correct_predictions += 1\n",
    "\n",
    "    return number_of_correct_predictions / total_number_of_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(X, Y, T):\n",
    "    T = T.astype(np.float64)\n",
    "    T /= timestamp_delta_generator(months=1)\n",
    "    T += np.random.rand(len(T)) * Y\n",
    "\n",
    "    index = np.argsort(T, axis=0).ravel()\n",
    "    X = X[index, :]\n",
    "    Y = Y[index]\n",
    "    T = T[index]\n",
    "\n",
    "    return X, Y, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: Model, X_train: np.ndarray, Y_train: np.ndarray, T_train: np.ndarray, X_test: np.ndarray,\n",
    "             Y_test: np.ndarray, T_test: np.ndarray):\n",
    "    model.fit(X_train, Y_train, T_train)\n",
    "\n",
    "    # T_pred = model.mean(X_test)\n",
    "    T_pred = model.quantile(X_test, .5).ravel()\n",
    "    T_pred = np.fmin(T_pred, max(T_test))\n",
    "\n",
    "    c_index = generate_c_index(T_test, T_pred, Y_test)\n",
    "\n",
    "    k = Y_test.sum()\n",
    "    # X_test = X_test[:k, :]\n",
    "    T_test = T_test[:k]\n",
    "    T_pred = T_pred[:k]\n",
    "\n",
    "#     threshold = [.5, 1, 1.5, 2, 2.5, 3]\n",
    "\n",
    "    # lb1 = model.quantile(X_test, .25).ravel()\n",
    "    # ub1 = model.quantile(X_test, .75).ravel()\n",
    "    #\n",
    "    # lb2 = model.quantile(X_test, .2).ravel()\n",
    "    # ub2 = model.quantile(X_test, .8).ravel()\n",
    "    #\n",
    "    # lb3 = model.quantile(X_test, .15).ravel()\n",
    "    # ub3 = model.quantile(X_test, .85).ravel()\n",
    "    #\n",
    "    # C1 = np.logical_and(lb1 <= T_test, T_test <= ub1)\n",
    "    # C2 = np.logical_and(lb2 <= T_test, T_test <= ub2)\n",
    "    # C3 = np.logical_and(lb3 <= T_test, T_test <= ub3)\n",
    "    #\n",
    "\n",
    "#     distance = np.zeros((len(threshold)))\n",
    "#     for i in range(len(threshold)):\n",
    "#         distance[i] = (res <= threshold[i]).sum() / len(res)\n",
    "\n",
    "#     ev = explained_variance_score(T_test, T_pred)\n",
    "    mae = mean_absolute_error(T_test, T_pred)\n",
    "    rmse = mean_squared_error(T_test, T_pred)**.5\n",
    "    msle = mean_squared_log_error(T_test, T_pred)\n",
    "    mad = median_absolute_error(T_test, T_pred)\n",
    "#     r2 = r2_score(T_test, T_pred)\n",
    "    # ci5 = C1.sum() / len(C1)\n",
    "    # ci6 = C2.sum() / len(C2)\n",
    "    # ci7 = C3.sum() / len(C3)\n",
    "\n",
    "    return mae, rmse, msle, mad, c_index #, distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(dists, X_stat, X, Y, T, cv=5):\n",
    "    threads = []\n",
    "    results = {dist+pos: [] for dist in dists for pos in ['', '_stat']}\n",
    "    k_fold = StratifiedKFold(n_splits=cv, shuffle=True)\n",
    "\n",
    "    for training_indices, test_indices in k_fold.split(X=X, y=Y):\n",
    "        X_stat_train = X_stat[training_indices, :]\n",
    "        X_train = X[training_indices, :]\n",
    "        Y_train = Y[training_indices]\n",
    "        T_train = T[training_indices]\n",
    "\n",
    "        X_stat_test = X_stat[test_indices, :]\n",
    "        X_test = X[test_indices, :]\n",
    "        Y_test = Y[test_indices]\n",
    "        T_test = T[test_indices]\n",
    "\n",
    "        def worker():\n",
    "            for dist in dists:\n",
    "                model = get_model(dist)\n",
    "                scores = evaluate(model, X_train, Y_train, T_train, X_test, Y_test, T_test)\n",
    "                results[dist].append(scores)\n",
    "                scores_stat = evaluate(model, X_stat_train, Y_train, T_train, X_stat_test, Y_test, T_test)\n",
    "                results[dist+'_stat'].append(scores_stat)\n",
    "\n",
    "        job = threading.Thread(target=worker)\n",
    "        threads.append(job)\n",
    "\n",
    "    for t in threads:\n",
    "        t.start()\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name(dist):\n",
    "    return {\n",
    "        'np': 'NP-Glm',\n",
    "        'wbl': 'Wbl-Glm',\n",
    "        'exp': 'Exp-Glm',\n",
    "        'ray': 'Ray-Glm',\n",
    "        'gom': 'Gom-Glm'\n",
    "    }[dist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:10:36: generating indexer ...\n",
      "16:10:37: parsing dataset ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2008-12-25 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:10:37: generating samples ...\n",
      "16:10:37: extracting ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:10:38: parsing dataset ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2008-09-23 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:10:38: extracting ...\n",
      "16:10:39: parsing dataset ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2008-06-22 01:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:10:39: extracting ...\n",
      "16:10:40: parsing dataset ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2008-03-21 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:10:40: extracting ...\n",
      "16:10:41: parsing dataset ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2007-12-19 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:10:41: extracting ...\n",
      "16:10:42: parsing dataset ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2007-09-17 01:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:10:42: extracting ...\n",
      "16:10:42: parsing dataset ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2007-06-16 01:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:10:43: extracting ...\n",
      "16:10:43: parsing dataset ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2007-03-15 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:10:44: extracting ...\n",
      "16:10:44: parsing dataset ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2006-12-12 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:10:45: extracting ...\n",
      "16:10:45: parsing dataset ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2006-09-10 01:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:10:46: extracting ...\n",
      "16:10:46: done.\n"
     ]
    }
   ],
   "source": [
    "X_list, Y_raw, T_raw = delicious_run(delta=3, observation_window=12, n_snapshots=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2596/2596 [==============================] - 2s 933us/step - loss: 5.7325\n",
      "Epoch 2/100\n",
      "2596/2596 [==============================] - 1s 227us/step - loss: 5.0457\n",
      "Epoch 3/100\n",
      "2596/2596 [==============================] - 1s 197us/step - loss: 4.8600\n",
      "Epoch 4/100\n",
      "2596/2596 [==============================] - 1s 204us/step - loss: 4.5557\n",
      "Epoch 5/100\n",
      "2596/2596 [==============================] - 1s 200us/step - loss: 3.4315\n",
      "Epoch 6/100\n",
      "2596/2596 [==============================] - 1s 218us/step - loss: 3.3399\n",
      "Epoch 7/100\n",
      "2596/2596 [==============================] - 1s 206us/step - loss: 3.2790\n",
      "Epoch 8/100\n",
      "2596/2596 [==============================] - 1s 205us/step - loss: 3.2311\n",
      "Epoch 9/100\n",
      "2596/2596 [==============================] - 1s 209us/step - loss: 3.1953\n",
      "Epoch 10/100\n",
      "2596/2596 [==============================] - 1s 212us/step - loss: 3.1715\n",
      "Epoch 11/100\n",
      "2596/2596 [==============================] - 1s 242us/step - loss: 3.1579\n",
      "Epoch 12/100\n",
      "2596/2596 [==============================] - 1s 217us/step - loss: 3.1510\n",
      "Epoch 13/100\n",
      "2596/2596 [==============================] - 1s 195us/step - loss: 3.1476\n",
      "Epoch 14/100\n",
      "2596/2596 [==============================] - 1s 211us/step - loss: 3.1455\n",
      "Epoch 15/100\n",
      "2596/2596 [==============================] - 1s 216us/step - loss: 3.1438\n",
      "Epoch 16/100\n",
      "2596/2596 [==============================] - 1s 218us/step - loss: 3.1423\n",
      "Epoch 17/100\n",
      "2596/2596 [==============================] - 1s 199us/step - loss: 3.1408\n",
      "Epoch 18/100\n",
      "2596/2596 [==============================] - 1s 213us/step - loss: 3.1394\n",
      "Epoch 19/100\n",
      "2596/2596 [==============================] - 1s 209us/step - loss: 3.1367\n",
      "Epoch 20/100\n",
      "2596/2596 [==============================] - 1s 254us/step - loss: 3.1298\n",
      "Epoch 21/100\n",
      "2596/2596 [==============================] - 1s 231us/step - loss: 3.0888\n",
      "Epoch 22/100\n",
      "2596/2596 [==============================] - 1s 231us/step - loss: 2.3021\n",
      "Epoch 23/100\n",
      "2596/2596 [==============================] - 1s 218us/step - loss: 1.9601\n",
      "Epoch 24/100\n",
      "2596/2596 [==============================] - 0s 192us/step - loss: 1.9391\n",
      "Epoch 25/100\n",
      "2596/2596 [==============================] - 1s 206us/step - loss: 1.9259\n",
      "Epoch 26/100\n",
      "2596/2596 [==============================] - 1s 257us/step - loss: 1.9172\n",
      "Epoch 27/100\n",
      "2596/2596 [==============================] - 1s 231us/step - loss: 1.9116\n",
      "Epoch 28/100\n",
      "2596/2596 [==============================] - 1s 200us/step - loss: 1.9080\n",
      "Epoch 29/100\n",
      "2596/2596 [==============================] - 1s 248us/step - loss: 1.9058\n",
      "Epoch 30/100\n",
      "2596/2596 [==============================] - 1s 225us/step - loss: 1.9044\n",
      "Epoch 31/100\n",
      "2596/2596 [==============================] - 1s 234us/step - loss: 1.9035\n",
      "Epoch 32/100\n",
      "2596/2596 [==============================] - 1s 198us/step - loss: 1.9029\n",
      "Epoch 33/100\n",
      "2596/2596 [==============================] - 1s 201us/step - loss: 1.9025\n",
      "Epoch 34/100\n",
      "2596/2596 [==============================] - 1s 229us/step - loss: 1.9022\n",
      "Epoch 35/100\n",
      "2596/2596 [==============================] - 1s 211us/step - loss: 1.9020\n",
      "Epoch 36/100\n",
      "2596/2596 [==============================] - 1s 232us/step - loss: 1.9018\n",
      "Epoch 37/100\n",
      "2596/2596 [==============================] - 1s 241us/step - loss: 1.9016\n",
      "Epoch 38/100\n",
      "2596/2596 [==============================] - 1s 274us/step - loss: 1.9014\n",
      "Epoch 39/100\n",
      "2596/2596 [==============================] - 1s 289us/step - loss: 1.9012\n",
      "Epoch 40/100\n",
      "2596/2596 [==============================] - 1s 271us/step - loss: 1.9010\n",
      "Epoch 41/100\n",
      "2596/2596 [==============================] - 1s 288us/step - loss: 1.9009\n",
      "Epoch 42/100\n",
      "2596/2596 [==============================] - 1s 297us/step - loss: 1.9007\n",
      "Epoch 43/100\n",
      "2596/2596 [==============================] - 1s 266us/step - loss: 1.9006\n",
      "Epoch 44/100\n",
      "2596/2596 [==============================] - 1s 293us/step - loss: 1.9004\n",
      "Epoch 45/100\n",
      "2596/2596 [==============================] - 1s 289us/step - loss: 1.9003\n",
      "Epoch 46/100\n",
      "2596/2596 [==============================] - 1s 253us/step - loss: 1.9002\n",
      "Epoch 47/100\n",
      "2596/2596 [==============================] - 1s 280us/step - loss: 1.9001\n",
      "Epoch 48/100\n",
      "2596/2596 [==============================] - 1s 282us/step - loss: 1.9000\n",
      "Epoch 49/100\n",
      "2596/2596 [==============================] - 1s 269us/step - loss: 1.8999\n",
      "Epoch 50/100\n",
      "2596/2596 [==============================] - 1s 257us/step - loss: 1.8998\n",
      "Epoch 51/100\n",
      "2596/2596 [==============================] - 1s 230us/step - loss: 1.8997\n",
      "Epoch 52/100\n",
      "2596/2596 [==============================] - 1s 225us/step - loss: 1.8996\n",
      "Epoch 53/100\n",
      "2596/2596 [==============================] - 1s 237us/step - loss: 1.8995\n",
      "Epoch 54/100\n",
      "2596/2596 [==============================] - 1s 241us/step - loss: 1.8994\n",
      "Epoch 55/100\n",
      "2596/2596 [==============================] - 1s 289us/step - loss: 1.8993\n",
      "Epoch 56/100\n",
      "2596/2596 [==============================] - 1s 235us/step - loss: 1.8991\n",
      "Epoch 57/100\n",
      "2596/2596 [==============================] - 1s 233us/step - loss: 1.8990\n",
      "Epoch 58/100\n",
      "2596/2596 [==============================] - 1s 223us/step - loss: 1.8989\n",
      "Epoch 59/100\n",
      "2596/2596 [==============================] - 1s 277us/step - loss: 1.8988\n",
      "Epoch 60/100\n",
      "2596/2596 [==============================] - 1s 233us/step - loss: 1.8986\n",
      "Epoch 61/100\n",
      "2596/2596 [==============================] - 1s 247us/step - loss: 1.8985\n",
      "Epoch 62/100\n",
      "2596/2596 [==============================] - 1s 247us/step - loss: 1.8984\n",
      "Epoch 63/100\n",
      "2596/2596 [==============================] - 1s 244us/step - loss: 1.8983\n",
      "Epoch 64/100\n",
      "2596/2596 [==============================] - 1s 244us/step - loss: 1.8983\n",
      "Epoch 65/100\n",
      "2596/2596 [==============================] - 1s 217us/step - loss: 1.8982\n",
      "Epoch 66/100\n",
      "2596/2596 [==============================] - 1s 239us/step - loss: 1.8981\n",
      "Epoch 67/100\n",
      "2596/2596 [==============================] - 1s 252us/step - loss: 1.8979\n",
      "Epoch 68/100\n",
      "2596/2596 [==============================] - 1s 289us/step - loss: 1.8977\n",
      "Epoch 69/100\n",
      "2596/2596 [==============================] - 1s 273us/step - loss: 1.8975\n",
      "Epoch 70/100\n",
      "2596/2596 [==============================] - 1s 285us/step - loss: 1.8973\n",
      "Epoch 71/100\n",
      "2596/2596 [==============================] - 1s 248us/step - loss: 1.8971\n",
      "Epoch 72/100\n",
      "2596/2596 [==============================] - 1s 229us/step - loss: 1.8969\n",
      "Epoch 73/100\n",
      "2596/2596 [==============================] - 1s 249us/step - loss: 1.8968\n",
      "Epoch 74/100\n",
      "2596/2596 [==============================] - 1s 266us/step - loss: 1.8966\n",
      "Epoch 75/100\n",
      "2596/2596 [==============================] - 1s 235us/step - loss: 1.8964\n",
      "Epoch 76/100\n",
      "2596/2596 [==============================] - 1s 245us/step - loss: 1.8967\n",
      "Epoch 77/100\n",
      "2596/2596 [==============================] - 1s 246us/step - loss: 1.8965\n",
      "Epoch 78/100\n",
      "2596/2596 [==============================] - 1s 225us/step - loss: 1.8959\n",
      "Epoch 79/100\n",
      "2596/2596 [==============================] - 1s 236us/step - loss: 1.8955\n",
      "Epoch 80/100\n",
      "2596/2596 [==============================] - 1s 241us/step - loss: 1.8953\n",
      "Epoch 81/100\n",
      "2596/2596 [==============================] - 1s 285us/step - loss: 1.8951\n",
      "Epoch 82/100\n",
      "2596/2596 [==============================] - 1s 299us/step - loss: 1.8947\n",
      "Epoch 83/100\n",
      "2596/2596 [==============================] - 1s 254us/step - loss: 1.8945\n",
      "Epoch 84/100\n",
      "2596/2596 [==============================] - 1s 243us/step - loss: 1.8941\n",
      "Epoch 85/100\n",
      "2596/2596 [==============================] - 1s 244us/step - loss: 1.8938\n",
      "Epoch 86/100\n",
      "2596/2596 [==============================] - 1s 245us/step - loss: 1.8934\n",
      "Epoch 87/100\n",
      "2596/2596 [==============================] - 1s 236us/step - loss: 1.8931\n",
      "Epoch 88/100\n",
      "2596/2596 [==============================] - 1s 320us/step - loss: 1.8926\n",
      "Epoch 89/100\n",
      "2596/2596 [==============================] - 1s 276us/step - loss: 1.8919\n",
      "Epoch 90/100\n",
      "2596/2596 [==============================] - 1s 297us/step - loss: 1.8914\n",
      "Epoch 91/100\n",
      "2596/2596 [==============================] - 1s 245us/step - loss: 1.8908\n",
      "Epoch 92/100\n",
      "2596/2596 [==============================] - 1s 241us/step - loss: 1.8900\n",
      "Epoch 93/100\n",
      "2596/2596 [==============================] - 1s 259us/step - loss: 1.8895\n",
      "Epoch 94/100\n",
      "2596/2596 [==============================] - 1s 239us/step - loss: 1.8887\n",
      "Epoch 95/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2596/2596 [==============================] - 1s 214us/step - loss: 1.8879\n",
      "Epoch 96/100\n",
      "2596/2596 [==============================] - 1s 214us/step - loss: 1.8872\n",
      "Epoch 97/100\n",
      "2596/2596 [==============================] - 1s 274us/step - loss: 1.8862\n",
      "Epoch 98/100\n",
      "2596/2596 [==============================] - 1s 217us/step - loss: 1.8851\n",
      "Epoch 99/100\n",
      "2596/2596 [==============================] - 1s 226us/step - loss: 1.8840\n",
      "Epoch 100/100\n",
      "2596/2596 [==============================] - 1s 209us/step - loss: 1.8828\n"
     ]
    }
   ],
   "source": [
    "X_raw = encode(X_list, Y_raw, epochs=100, latent_factor=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2596\n",
      "NP-Glm\n",
      "MAE=5.00\tRMSE=6.14\tMSLE=0.89\tMAD=4.36\tCI=0.69\n",
      "Wbl-Glm\n",
      "MAE=5.07\tRMSE=6.23\tMSLE=0.89\tMAD=4.15\tCI=0.58\n",
      "NP-Glm_stat\n",
      "MAE=5.46\tRMSE=6.48\tMSLE=1.10\tMAD=5.40\tCI=0.63\n",
      "Wbl-Glm_stat\n",
      "MAE=5.56\tRMSE=6.56\tMSLE=1.10\tMAD=4.93\tCI=0.54\n"
     ]
    }
   ],
   "source": [
    "print(len(T))\n",
    "X, Y, T = prepare_data(X_raw, Y_raw, T_raw)\n",
    "scaler = MinMaxScaler(copy=True)\n",
    "X_stat = scaler.fit_transform(X_list[-1])\n",
    "\n",
    "dists = [\n",
    "    'np',\n",
    "    'wbl',\n",
    "#     'exp',\n",
    "#     'ray',\n",
    "    # 'gom'\n",
    "]\n",
    "\n",
    "results = cross_validate(dists, X_stat, X, Y, T, cv=5)\n",
    "#         ev, mae, rmse, msle, mad, r2, c_index #, distance\n",
    "for pos in ['', '_stat']:\n",
    "    for dist in dists:\n",
    "        print(get_name(dist)+pos)\n",
    "        result = np.array(results[dist+pos])\n",
    "        mean = result.mean(axis=0)\n",
    "    #     std = result.std(axis=0)\n",
    "        print('MAE=%.2f\\tRMSE=%.2f\\tMSLE=%.2f\\tMAD=%.2f\\tCI=%.2f' % tuple(mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
